---
title: "Semana 0 - Creando la wiki"
image: ../assets/images/logo.png
categories:
  - Weekly Log
tags:
  - github pages
---

En la primera semana, comenzamos instalando ROS y todos los paquetes necesarios. Además, empecé a leer papers académicos para familiarizarme con el contexto del proyecto.

## Instalación de ROS y Gazebo

Primero, instalamos el software básico para el desarrollo del proyecto siguiendo los pasos de instalación de los sitios web oficiales: [ROS](https://docs.ros.org/en/humble/Installation/Ubuntu-Install-Debians.html), [Gazebo](https://classic.gazebosim.org/tutorials?tut=install_ubuntu), adittionally we instaled [Aereostack](https://aerostack2.github.io/_00_getting_started/source_install.html)


## Papers
### Infraestructura de programación de robots aéreos y aplicaciones visuales con aprendizaje profundo

El [TFM](https://gsyc.urjc.es/jmplaza/students/tfm-drones-followperson-pedro_arias-2022.pdf) de Pedro Arias Pérez tiene bastantes puntos importantes, para el TFG nos centraremos en las herramientas que se usan para el control del drone, los drivers y el apartado de visión artificial.

En el punto 2, referente a las herramientas utilizadas, encontramos información interesante como varios simuladores en programación de drones, plataformas donde desarrollar aplicaciones de aprendizaje automático y el bucle estándar de control en autopilotos:

<figure class="align-center" style="width:70%">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/images/post0/autopilotControl.png" alt="">
  <figcaption>Autopilot control</figcaption>
</figure>

Respecto a los materiales, nos centraremos en el punto que se refiere al dron DJI Tello y la simulación realizada, que en este caso se realiza mediante el SITL de PX4 (el otro dron utilizado) sobre el simulador Gazebo 9.

En la infraestructura desarrolladora cabe destacar el [Tello Driver](https://github.com/JdeRobot/drones/tree/melodic-devel), aunque esté en ROS nos puede dar una idea del funcionamiento de los drivers. La arquitectura de comunicación es la siguiente: 

<figure class="align-center" style="width:70%">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/images/post0/telloDriver.png" alt="">
  <figcaption>Infraestructura del driver</figcaption>
</figure>

* ***CmdVelSocket***: Se utiliza para el envío y recepción de comandos.
* ***StateSocket***: Se utiliza para recibir la información del estado de la nave.
* ***VideoCapture***: Se encarga de recibir las imágenes recibidas por el Tello.
* ***MAVROS***: La comunicación se realiza a través de ocho topics, siete publicadores y un suscriptor, y seis servicios.

Para la cámara se utiliza la Victure Driver, todo el código se encuentra en este [repositorio](https://github.com/RoboticsLabURJC/2021-tfm-pedro-arias/tree/main).

### Conducción autónoma de un vehículo en simulador mediante aprendizaje extremo a extremo basado en visión

El [TFM](https://gsyc.urjc.es/jmplaza/students/tfm-deeplearning_autonomous_navigation-vanessa-2019.pdf) de Vanessa Fernández Martínez nos pone en contexto del entrenamiento de redes neuronales.

En el primer punto se mencionan aspectos como las redes neuronales convolucionales (usadas principalmente para visión), recurrentes (donde se usa la información de manera secuencial) y LSTM, que es un tipo de las mismas. También me parecieron interesantes los tipos de capas:

* ***Capa convolucional***: Recibe como entrada una imagen y luego aplica sobre ella un filtro o kernel que devuelve una capa de características.

* ***Capa de Pooling***: Normalmente se coloca detrás de la capa convolucional y se emplea para reducir las dimensiones espaciales, pero no afecta a la dimensión de profundidad del volumen.

* ***Capa fully conected***: Conecta cada neurona de la capa de entrada con cada neurona de la capa de salida.

* ***Capa LSTM***: Puede añadir o quitar información a través de estructuras llamadas puertas, que funcionan como un mecanismo de control para regular el flujo de información, permitiendo que ciertas señales pasen mientras bloquean otras.

En el tercer punto hay que destacar la obtención del dataset para entrenar la red neuronal. También se explica la red PilotNet:

<figure class="align-center" style="width:70%">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/images/post0/pilotNet.png" alt="">
  <figcaption>PilotNet</figcaption>
</figure>

También es importante mencionar que las arquitecturas ResNet son utilizadas como extractores de características en el contexto del problema de regresión, especialmente cuando se consideran solo las capas convolucionales de estas redes.

En el TFM se predice el ángulo empleando 3 tipos de entradas:
1. Imágenes en escala de grises.
2. Diferencia de imágenes en escala de grises.
3. Imágenes creadas por la acumulación de eventos.

En el apartado 4, se crea su propio dataset a partir de un piloto autónomo y se mencionan aspectos importantes a la hora de desarrollar modelos de Deep Learning, como la división de los datos en un subconjunto de entrenamiento, datos de validación y datos de prueba.

La red realiza diferentes clasificaciones según los valores de los ángulos y las velocidades lineales. En los experimentos se realizan varias combinaciones de velocidad de tracción (v) y velocidad de rotación (w). Durante la conducción, la red predecirá una determinada clase tanto para v como para w.

#### Conceptos
* ***Ventana deslizante***: Permite a la red reconocer diferentes ángulos de dirección desde el mismo fotograma pero en diferentes estados temporales de las capas LSTM.

* ***Middleware neural Keras***: Middleware de alto nivel para redes neuronales, capaz de ejecutarse sobre TensorFlow, CNTK o Theano, con el objetivo de facilitar y acelerar la implementación de modelos de aprendizaje profundo en investigación y desarrollo.

* ***Formato HDF5***: Hierarchichal Data Format, es una librería de propósito general y, al mismo tiempo, un formato de archivo para el almacenamiento de datos científicos.

* ***Método stacked frames***: Se concatenan varias imágenes de entrada consecutivas para crear una imagen ampliada.

* ***ControlNet***: Esta arquitectura de red neuronal utiliza imágenes RGB como entrada para generar comandos de control. Combina capas convolucionales con capas de maxpooling, seguidas de capas completamente conectadas. Las capas convolucionales y de pooling extraen información geométrica del entorno, mientras que las capas completamente conectadas actúan como un clasificador general. La capa LSTM permite que el robot incorpore información temporal, lo que le permite mantener su movimiento en la misma dirección a lo largo de varios fotogramas.

<figure class="align-center" style="width:70%">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/images/post0/controlNet.png" alt="">
  <figcaption>ControlNet</figcaption>
</figure>


### Aprendiendo a explorar entornos interiores utilizando vehículos aéreos autónomos

Encontré un [paper](https://arxiv.org/abs/2309.06986) en internet que también trataba sobre drones, un aspetcto interesante es que usa deeplearning para predicción a la hora de generar el mapa, la arquitectura también es interesante:

<figure class="align-center" style="width:70%">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/images/post0/droneArchitecture.png" alt="">
  <figcaption>Arquitectura drone</figcaption>
</figure>

Para el predictor de mapa diseñaron una función de recompensa. Para la exploración se utiliza una red neuroanl DRL, la entrada es un mapa de 2 capas pasando por 3 capas convolucionales con activación ReLU.

